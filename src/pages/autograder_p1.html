<p>After setting up your environment in<span>&nbsp;</span><a title="Environment Setup" href="https://bruinlearn.ucla.edu/courses/214025/pages/environment-setup" data-course-type="wikiPages" data-published="true" data-api-endpoint="https://bruinlearn.ucla.edu/api/v1/courses/214025/pages/environment-setup" data-api-returntype="Page"><strong>Environment Setup</strong></a>, we can now demonstrate how to use the local autograding harness. Make sure Docker Desktop is running (or OrbStack if on macOS). If you haven&rsquo;t already, clone the<span>&nbsp;</span><a class="inline_disabled" href="https://github.com/uclacs118/project1starter" target="_blank" rel="noopener"><strong>Starter Code</strong></a><span>&nbsp;</span>onto your machine.</p>
<h1 id="autograding" class="anchor">Autograding</h1>
<p>In the main directory, there&rsquo;s a<span>&nbsp;</span><code>helper</code><span>&nbsp;</span>tool we&rsquo;ll be using to interface with the local autograder. To start, run<span>&nbsp;</span><code>./helper run</code>. This will pull the Docker image from our registry, build your submission (in the<span>&nbsp;</span><code>project</code><span>&nbsp;</span>directory), and run the autograding code.</p>
<p>After it&rsquo;s completed, you&rsquo;ll see JSON formatted output of your results. You should be failing all the test cases since there&rsquo;s only the starter code. See<span>&nbsp;</span>Autograder Test Cases<span>&nbsp;</span>for more info. This is also available in<span>&nbsp;</span><code>results &gt; results.json</code>.</p>
<p>You may also see the output of<span>&nbsp;</span><code>stderr</code><span>&nbsp;</span>for each run. Each file in the<span>&nbsp;</span><code>results</code><span>&nbsp;</span>directory will have the test case name (example:<span>&nbsp;</span><code>test_self_ascii</code>) appended with either<span>&nbsp;</span><code>refserver</code>,<span>&nbsp;</span><code>refclient</code>,<span>&nbsp;</span><code>yourserver</code>, and<span>&nbsp;</span><code>yourclient</code>.</p>
<h1 id="custom-testing" class="anchor">Custom Testing</h1>
<p>You may run each test case individually. For example, take this test case from Project 0:</p>
<ol>
    <li><strong>Data Transport (Your Client &lt;-&gt; Your Server): Small, ASCII only file (10 KB)<br /><code>test_self_ascii</code><span>&nbsp;</span>(10 points)</strong></li>
</ol>
<p>To test just this test case, run<span>&nbsp;</span><code>./helper test test_self_ascii</code>. You may test compilation by either running<span>&nbsp;</span><code>./helper test compile</code><span>&nbsp;</span>or<span>&nbsp;</span><code>./helper compile</code><span>&nbsp;</span>(the former runs the compilation code within the autograder and the latter just runs<span>&nbsp;</span><code>make</code>. Only the latter will show the compilation output).</p>
<p>Another way to run tests is to make your own. Run<span>&nbsp;</span><code>./helper interactive</code>. This will drop you into the autograding harness&rsquo;s shell.<span>&nbsp;</span><strong>Running<span>&nbsp;</span><code>./helper interactive</code><span>&nbsp;</span>in another terminal window will use the same autograder-if you want to test two binaries against each other, this is the best way to do it.</strong></p>
<p>In the<span>&nbsp;</span><code>/autograder</code><span>&nbsp;</span>directory, there are three important subdirectories.</p>
<p>One of them is<span>&nbsp;</span><code>/autograder/source</code>. This is where all the autograding logic resides. In the<span>&nbsp;</span><code>src</code><span>&nbsp;</span>folder, there are the reference binaries. Feel free to test your solutions against them.</p>
<p>Another is<span>&nbsp;</span><code>/autograder/submission</code>. This is linked directly to the<span>&nbsp;</span><code>project</code><span>&nbsp;</span>directory on your local machine (any file you change in<span>&nbsp;</span><code>project</code><span>&nbsp;</span>changes in<span>&nbsp;</span><code>submission</code><span>&nbsp;</span>in the autograder and vice-versa).</p>
<p>The last one is<span>&nbsp;</span><code>/autograder/results</code>. Just like<span>&nbsp;</span><code>submission</code>, it&rsquo;s linked to the<span>&nbsp;</span><code>results</code><span>&nbsp;</span>directory on your local machine.</p>
<h1 id="manually-running-the-executables" class="anchor">Manually Running the Executables</h1>
<p>In two separate windows, use<span>&nbsp;</span><code>./helper interactive</code>.</p>
<ol>
    <li>In one window, create a file of random length. Run<span>&nbsp;</span><code>head -c 20000 /dev/urandom &gt; /tmp/test.bin</code>. Feel free to change the size (in bytes).</li>
    <li>In both windows, navigate to either<span>&nbsp;</span><code>/autograder/source/src</code><span>&nbsp;</span>for the reference binaries or<span>&nbsp;</span><code>/autograder/submission</code><span>&nbsp;</span>for your submission.</li>
    <li>If using your submission, make sure to<span>&nbsp;</span><code>make</code><span>&nbsp;</span>first.</li>
    <li>Use file redirection to simulate a test case. For example, try<span>&nbsp;</span><code>./server 8080 &lt; /tmp/test.bin &gt; server.bin</code><span>&nbsp;</span>in one window and<span>&nbsp;</span><code>./client localhost 8080 &lt; /tmp/test.bin &gt; client.bin</code><span>&nbsp;</span>in another.</li>
    <li>Once the file has completed transferring on both sides, exit using<span>&nbsp;</span><code>Ctrl-C</code><span>&nbsp;</span>and check if the transfer completed successfully. Use<span>&nbsp;</span><code>diff /tmp/test.bin server.bin</code>. If there&rsquo;s no output, the files are the same; the transfer was successful. Try with the client as well.</li>
</ol>
<h1 id="manually-running-the-executables" class="anchor">Helper Script</h1>
<p>We may want to update the<span>&nbsp;</span><code>helper</code><span>&nbsp;</span>script to add more helpful utilities. Run<span>&nbsp;</span><code>git pull origin main</code><span>&nbsp;</span>to update it.</p>
